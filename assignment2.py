# -*- coding: utf-8 -*-
"""assignment2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HU8wTIeFGPTj4Wc16XRmagVa4rbPUi2G
"""

import pandas as pd
import numpy as np

data=pd.read_csv("melb_data.csv")

data.head()

data.info()

features = data.iloc[:,[12, 13, 14, 15]].values
label = data.iloc[:,[4]].values

feature_new= pd.DataFrame(features, columns= ["Car", "Landsize", "BuildingArea", "YearBuilt"])
label_new= pd.DataFrame(label, columns= ['Price'])

feature_new.head()

feature_new.info()

feature_new.isnull().sum()

feature_new["Car"].value_counts()

feature_new[feature_new["Car"]==0].sample(5)

import seaborn as sns

# # prompt: remove most extreme outliers from "Landsize", "BuildingArea", "YearBuilt"

# for column in ["Landsize", "BuildingArea", "YearBuilt"]:
#   Q1 = feature_new[column].quantile(0.25)
#   Q3 = feature_new[column].quantile(0.75)
#   IQR = Q3 - Q1
#   lower_bound = Q1 - 200* IQR
#   upper_bound = Q3 + 200* IQR
#   feature_new_change = feature_new[(feature_new[column] >= lower_bound) & (feature_new[column] <= upper_bound)]

# feature_new_change.info()

# # prompt: how many datapoints were removed as outliers

# # Store the original number of rows
# original_rows = 13580  # Based on data.info() output in the provided code

# # The number of rows after removing outliers is feature_new.shape[0]
# rows_after_outliers = feature_new_change.shape[0]

# # Calculate the number of removed data points
# removed_datapoints = original_rows - rows_after_outliers

# print(f"Number of datapoints removed as outliers: {removed_datapoints}")

#sns.pairplot(feature_new_change)

feature_new["Car"].fillna(feature_new["Car"].mode()[0], inplace=True)
feature_new["Landsize"].fillna(feature_new["Landsize"].mean(), inplace=True)
feature_new["BuildingArea"].fillna(feature_new["BuildingArea"].mean(), inplace=True)
feature_new['YearBuilt'].fillna(feature_new['YearBuilt'].mode()[0], inplace=True)

feature_new.isnull().sum()

feature_new.info()

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from xgboost import XGBRFRegressor

import warnings
warnings.filterwarnings("ignore")

def run_model(features, label, model):
    CL =0.4

    for seed in range(1,100):
      X_train,X_test,y_train,y_test = train_test_split(features,
                                                      label,
                                                      test_size=0.2,
                                                      random_state=seed)
      model.fit(X_train,y_train)

      testScore = model.score(X_test,y_test)
      trainScore = model.score(X_train,y_train)

      if testScore > trainScore and testScore >= CL:
        print(f"Model name: {model}, Test Score {testScore} | Train Score {trainScore} | RandomState used {seed}")

feature_new=np.array(feature_new)
label_new=np.array(label_new)

for model in [ XGBRFRegressor()]:
  run_model(feature_new, label_new, model)

X_train,X_test,y_train,y_test = train_test_split(features,
                                                      label,
                                                      test_size=0.2,
                                                      random_state=61)

model=XGBRFRegressor()
model.fit(X_train,y_train)

print("Training Score is :" ,model.score(X_train,y_train))
print("Test Score is :" ,model.score(X_test,y_test))

import pickle
pickle.dump(model, open("housingprediction.pkl","wb"))

